{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2018 Les auteurs de TensorFlow."
      ],
      "metadata": {
      }
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "outputs": [

      ],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/guide/saved_model\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Voir sur TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/saved_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Exécuter dans Google Colab</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/saved_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Voir la source sur GitHub</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/guide/saved_model.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Télécharger le cahier</a></td>\n",
        "</table>"
      ],
      "metadata": {
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un SavedModel contient un programme TensorFlow complet, y compris les pondérations et le calcul. Il ne nécessite pas l'exécution du code de construction du modèle d'origine, ce qui le rend utile pour le partage ou le déploiement (avec [TFLite](https://tensorflow.org/lite) , [TensorFlow.js](https://js.tensorflow.org/) , [TensorFlow Serving](https://www.tensorflow.org/tfx/serving/tutorials/Serving_REST_simple) ou [TensorFlow Hub](https://tensorflow.org/hub) ).\n",
        "\n",
        "Ce document plonge dans certains des détails de l'utilisation de l'API de bas niveau `tf.saved_model` :\n",
        "\n",
        "- Si vous utilisez un `tf.keras.Model` la `keras.Model.save(output_path)` peut être tout ce dont vous avez besoin : Voir la [sauvegarde et la sérialisation de Keras](keras/save_and_serialize.ipynb)\n",
        "\n",
        "- Si vous souhaitez simplement enregistrer/charger des poids pendant l'entraînement, consultez le [guide des points de contrôle de l'entraînement](./checkpoint.ipynb) .\n"
      ],
      "metadata": {
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Création d'un modèle enregistré à partir de Keras\n",
        "\n",
        "Pour une introduction rapide, cette section exporte un modèle Keras pré-formé et sert les demandes de classification d'images avec celui-ci. Le reste du guide fournira des détails et discutera d'autres façons de créer des SavedModels."
      ],
      "metadata": {
      }
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "outputs": [

      ],
      "source": [
        "import os\n",
        "import tempfile\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "tmpdir = tempfile.mkdtemp()"
      ],
      "metadata": {
      }
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "outputs": [

      ],
      "source": [
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "  tf.config.experimental.set_memory_growth(physical_devices[0], True)"
      ],
      "metadata": {
      }
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "outputs": [

      ],
      "source": [
        "file = tf.keras.utils.get_file(\n",
        "    \"grace_hopper.jpg\",\n",
        "    \"https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg\")\n",
        "img = tf.keras.preprocessing.image.load_img(file, target_size=[224, 224])\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "x = tf.keras.preprocessing.image.img_to_array(img)\n",
        "x = tf.keras.applications.mobilenet.preprocess_input(\n",
        "    x[tf.newaxis,...])"
      ],
      "metadata": {
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"saved_model_cli\"></a>\n",
        "\n",
        "## Détails de l'interface de ligne de commande SavedModel\n",
        "\n",
        "Vous pouvez utiliser l'interface de ligne de commande (CLI) SavedModel pour inspecter et exécuter un SavedModel. Par exemple, vous pouvez utiliser la CLI pour inspecter les `SignatureDef` s du modèle. La CLI vous permet de confirmer rapidement que le dtype et la forme du tenseur d'entrée correspondent au modèle. De plus, si vous souhaitez tester votre modèle, vous pouvez utiliser la CLI pour effectuer une vérification de cohérence en transmettant des exemples d'entrées dans différents formats (par exemple, des expressions Python), puis en récupérant la sortie.\n",
        "\n",
        "### Installer l'interface de ligne de commande SavedModel\n",
        "\n",
        "D'une manière générale, vous pouvez installer TensorFlow de l'une des deux manières suivantes :\n",
        "\n",
        "- En installant un binaire TensorFlow pré-construit.\n",
        "- En créant TensorFlow à partir du code source.\n",
        "\n",
        "Si vous avez installé TensorFlow via un binaire TensorFlow pré-construit, la CLI SavedModel est déjà installée sur votre système au nom de chemin `bin/saved_model_cli` .\n",
        "\n",
        "Si vous avez compilé TensorFlow à partir du code source, vous devez exécuter la commande supplémentaire suivante pour compiler `saved_model_cli` :\n",
        "\n",
        "```\n",
        "$ bazel build tensorflow/python/tools:saved_model_cli\n",
        "```\n",
        "\n",
        "### Présentation des commandes\n",
        "\n",
        "La CLI SavedModel prend en charge les deux commandes suivantes sur un SavedModel :\n",
        "\n",
        "- `show` , qui affiche les calculs disponibles à partir d'un SavedModel.\n",
        "- `run` , qui exécute un calcul à partir d'un SavedModel.\n",
        "\n",
        "### `show` la commande\n",
        "\n",
        "Un SavedModel contient une ou plusieurs variantes de modèle (techniquement, `v1.MetaGraphDef` s), identifiées par leurs ensembles de balises. Pour servir un modèle, vous pourriez vous demander quel type de `SignatureDef` s sont dans chaque variante de modèle, et quelles sont leurs entrées et sorties. La commande `show` vous permet d'examiner le contenu du SavedModel dans l'ordre hiérarchique. Voici la syntaxe :\n",
        "\n",
        "```\n",
        "usage: saved_model_cli show [-h] --dir DIR [--all]\n",
        "[--tag_set TAG_SET] [--signature_def SIGNATURE_DEF_KEY]\n",
        "```\n",
        "\n",
        "Par exemple, la commande suivante affiche tous les ensembles de balises disponibles dans le SavedModel :\n",
        "\n",
        "```\n",
        "$ saved_model_cli show --dir /tmp/saved_model_dir\n",
        "The given SavedModel contains the following tag-sets:\n",
        "serve\n",
        "serve, gpu\n",
        "```\n",
        "\n",
        "La commande suivante affiche toutes les clés `SignatureDef` disponibles pour un ensemble de balises :\n",
        "\n",
        "```\n",
        "$ saved_model_cli show --dir /tmp/saved_model_dir --tag_set serve\n",
        "The given SavedModel `MetaGraphDef` contains `SignatureDefs` with the\n",
        "following keys:\n",
        "SignatureDef key: \"classify_x2_to_y3\"\n",
        "SignatureDef key: \"classify_x_to_y\"\n",
        "SignatureDef key: \"regress_x2_to_y3\"\n",
        "SignatureDef key: \"regress_x_to_y\"\n",
        "SignatureDef key: \"regress_x_to_y2\"\n",
        "SignatureDef key: \"serving_default\"\n",
        "```\n",
        "\n",
        "S'il y a *plusieurs* balises dans le jeu de balises, vous devez spécifier toutes les balises, chaque balise étant séparée par une virgule. Par example:\n",
        "\n",
        "<pre>$ saved_model_cli show --dir /tmp/saved_model_dir --tag_set serve,gpu\n",
        "</pre>\n",
        "\n",
        "Pour afficher toutes les entrées et sorties TensorInfo pour un `SignatureDef` spécifique, passez la clé `SignatureDef` à l'option `signature_def` . Ceci est très utile lorsque vous souhaitez connaître la valeur de la clé du tenseur, le dtype et la forme des tenseurs d'entrée pour exécuter le graphe de calcul ultérieurement. Par example:\n",
        "\n",
        "```\n",
        "$ saved_model_cli show --dir \\\n",
        "/tmp/saved_model_dir --tag_set serve --signature_def serving_default\n",
        "The given SavedModel SignatureDef contains the following input(s):\n",
        "  inputs['x'] tensor_info:\n",
        "      dtype: DT_FLOAT\n",
        "      shape: (-1, 1)\n",
        "      name: x:0\n",
        "The given SavedModel SignatureDef contains the following output(s):\n",
        "  outputs['y'] tensor_info:\n",
        "      dtype: DT_FLOAT\n",
        "      shape: (-1, 1)\n",
        "      name: y:0\n",
        "Method name is: tensorflow/serving/predict\n",
        "```\n",
        "\n",
        "Pour afficher toutes les informations disponibles dans SavedModel, utilisez l'option `--all` . Par example:\n",
        "\n",
        "<pre>$ saved_model_cli show --dir /tmp/saved_model_dir --all\n",
        "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
        "\n",
        "signature_def['classify_x2_to_y3']:\n",
        "  The given SavedModel SignatureDef contains the following input(s):\n",
        "    inputs['inputs'] tensor_info:\n",
        "        dtype: DT_FLOAT\n",
        "        shape: (-1, 1)\n",
        "        name: x2:0\n",
        "  The given SavedModel SignatureDef contains the following output(s):\n",
        "    outputs['scores'] tensor_info:\n",
        "        dtype: DT_FLOAT\n",
        "        shape: (-1, 1)\n",
        "        name: y3:0\n",
        "  Method name is: tensorflow/serving/classify\n",
        "\n",
        "...\n",
        "\n",
        "signature_def['serving_default']:\n",
        "  The given SavedModel SignatureDef contains the following input(s):\n",
        "    inputs['x'] tensor_info:\n",
        "        dtype: DT_FLOAT\n",
        "        shape: (-1, 1)\n",
        "        name: x:0\n",
        "  The given SavedModel SignatureDef contains the following output(s):\n",
        "    outputs['y'] tensor_info:\n",
        "        dtype: DT_FLOAT\n",
        "        shape: (-1, 1)\n",
        "        name: y:0\n",
        "  Method name is: tensorflow/serving/predict\n",
        "</pre>\n",
        "\n",
        "### `run` commande\n",
        "\n",
        "Appelez la commande `run` pour exécuter un calcul de graphe, en transmettant les entrées, puis en affichant (et éventuellement en enregistrant) les sorties. Voici la syntaxe :\n",
        "\n",
        "```\n",
        "usage: saved_model_cli run [-h] --dir DIR --tag_set TAG_SET --signature_def\n",
        "                           SIGNATURE_DEF_KEY [--inputs INPUTS]\n",
        "                           [--input_exprs INPUT_EXPRS]\n",
        "                           [--input_examples INPUT_EXAMPLES] [--outdir OUTDIR]\n",
        "                           [--overwrite] [--tf_debug]\n",
        "```\n",
        "\n",
        "La commande `run` fournit les trois manières suivantes de transmettre des entrées au modèle :\n",
        "\n",
        "- L'option `--inputs` vous permet de passer numpy ndarray dans les fichiers.\n",
        "- L'option `--input_exprs` vous permet de transmettre des expressions Python.\n",
        "- L'option `--input_examples` vous permet de passer `tf.train.Example` .\n",
        "\n",
        "#### `--inputs`\n",
        "\n",
        "Pour transmettre des données d'entrée dans des fichiers, spécifiez l'option `--inputs` , qui prend le format général suivant :\n",
        "\n",
        "```bsh\n",
        "--inputs <INPUTS>\n",
        "```\n",
        "\n",
        "où *INPUTS* est l'un des formats suivants :\n",
        "\n",
        "- `<input_key>=<filename>`\n",
        "- `<input_key>=<filename>[<variable_name>]`\n",
        "\n",
        "Vous pouvez passer plusieurs *INPUTS* . Si vous transmettez plusieurs entrées, utilisez un point-virgule pour séparer chacune des *INPUTS* .\n",
        "\n",
        "`saved_model_cli` utilise `numpy.load` pour charger le *filename* . Le nom de *fichier* peut être dans l'un des formats suivants :\n",
        "\n",
        "- `.npy`\n",
        "- `.npz`\n",
        "- format cornichon\n",
        "\n",
        "Un fichier `.npy` contient toujours un ndarray numpy. Par conséquent, lors du chargement à partir d'un fichier `.npy` , le contenu sera directement affecté au tenseur d'entrée spécifié. Si vous spécifiez un *variable_name* avec ce fichier `.npy` , le *variable_name* sera ignoré et un avertissement sera émis.\n",
        "\n",
        "Lors du chargement à partir d'un `.npz` (zip), vous pouvez éventuellement spécifier un *variable_name* pour identifier la variable dans le fichier zip à charger pour la clé de tenseur d'entrée. Si vous ne spécifiez pas de *variable_name* , la CLI SavedModel vérifiera qu'un seul fichier est inclus dans le fichier zip et le chargera pour la clé de tenseur d'entrée spécifiée.\n",
        "\n",
        "Lors du chargement à partir d'un fichier pickle, si aucun `variable_name` n'est spécifié entre crochets, tout ce qui se trouve à l'intérieur du fichier pickle sera transmis à la clé de tenseur d'entrée spécifiée. Sinon, la CLI SavedModel supposera qu'un dictionnaire est stocké dans le fichier pickle et la valeur correspondant au *variable_name* sera utilisée.\n",
        "\n",
        "#### `--input_exprs`\n",
        "\n",
        "Pour transmettre des entrées via des expressions Python, spécifiez l'option `--input_exprs` . Cela peut être utile lorsque vous n'avez pas de fichiers de données, mais que vous souhaitez tout de même vérifier l'intégrité du modèle avec quelques entrées simples qui correspondent au dtype et à la forme des `SignatureDef` s du modèle. Par example:\n",
        "\n",
        "```bsh\n",
        "`<input_key>=[[1],[2],[3]]`\n",
        "```\n",
        "\n",
        "En plus des expressions Python, vous pouvez également passer des fonctions numpy. Par example:\n",
        "\n",
        "```bsh\n",
        "`<input_key>=np.ones((32,32,3))`\n",
        "```\n",
        "\n",
        "(Notez que le module `numpy` est déjà disponible pour vous en tant que `np` .)\n",
        "\n",
        "#### `--input_examples`\n",
        "\n",
        "Pour passer `tf.train.Example` comme entrées, spécifiez l'option `--input_examples` . Pour chaque clé d'entrée, il faut une liste de dictionnaires, où chaque dictionnaire est une instance de `tf.train.Example` . Les clés du dictionnaire sont les fonctionnalités et les valeurs sont les listes de valeurs pour chaque fonctionnalité. Par example:\n",
        "\n",
        "```bsh\n",
        "`<input_key>=[{\"age\":[22,24],\"education\":[\"BS\",\"MS\"]}]`\n",
        "```\n",
        "\n",
        "#### Enregistrer la sortie\n",
        "\n",
        "Par défaut, la CLI SavedModel écrit la sortie sur stdout. Si un répertoire est passé à l'option `--outdir` , les sorties seront enregistrées en tant que fichiers `.npy` nommés d'après les clés de tenseur de sortie sous le répertoire donné.\n",
        "\n",
        "Utilisez `--overwrite` pour écraser les fichiers de sortie existants.\n"
      ],
      "metadata": {
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [

      ],
      "name": "saved_model.ipynb",
      "private_outputs": true,
      "provenance": [

      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
